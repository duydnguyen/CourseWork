\documentclass{article}
\usepackage{amsmath,amsxtra,amssymb,latexsym, amscd,amsthm}
\usepackage{indentfirst}
\usepackage[mathscr]{eucal}
\usepackage[margin=1.0in]{geometry}
\usepackage{subcaption}
\begin{document}
\title{Latent Class Regression on QTL Mapping}         % Ten bai
\author{Duy Nguyen}        % Tac gia
\date{02-18-2014}          % Ngay
\maketitle
\section*{Project Proposal}
In QTL literature \cite{Broman},  we consider a single backcross individual, and let $y$ denote its phenotype and $\mathbf{g}$ its whole  genome genotype. There are $p$ sites that matter in producing the phenotype and let $g_1,\ldots,g_p$ denote the individual's genotype at these $p$ QTL. We then have $E(y|g)=\mu_{g_1\ldots g_p}$ and $var(y|g)=\sigma^2_{g_1\ldots g_p}.$  We often assume that the QTL act additively, so that $E(y|g)=\mu_{g_1\ldots g_p} + \sum_{j} \Delta_j z_j,$ where $z_j=0$ or $1,$ according to whether $g_j$ is AA or AB. Second, we may assume $y|g $ is distributed as $ N(u_{g_1\ldots g_p}, \sigma^2).$ \\

In this project, I will apply latent class regression (LCR) models to QTL mapping. First, serving the same purpose as the described additive model above, LCR is used to predict a dependent variable (phenotype) as a function of predictors (genotype). Second, LCR includes an K-category latent variable, each category representing a homogenous population (class, segment). This is a significant aspect of LCR for which the additive model does not account when modeling the relationship between phenotype and genotypes. It relaxes the assumption that the same model holds for all cases $K=1.$ Further, different regressions are estimated for each population (for each latent segment). LCR model classifies cases into segments and develops regression models simultaneously.   \\

However, in LCR, computations are very involved since it uses EM algorithm to estimate the maximum likelihood estimates as discussed in \cite{Leisch}. In the course of this project, I propose the two possible approaches to solve this problem.\\

\textbf{Approach 1}: I will plan to take Shuyun Ye's approach. More specifically, I will simulate some data and analyze the power and false discovery rate (FDR) in various settings. \\

\textbf{Approach 2}: As discussed in \cite{Leisch}, it is well-known that the convergence of EM algorithm  can be slow and is to a local maximum of the likelihood surface only.  To solve this problem, I intend to take a different approach using machine learning framework. Initially, our  trained data set contains $(y_i,g_i), i=1,\ldots,n.$ Our goal is to classify the data into $k$ categories given that the data set does not contain information about labels (categories). This is unsupervised learning in machine learning. My main contribution is that I will develop a multi-class classifier which has an exponential rate of convergence. This idea is based on my recent work in fast-rate classifier in supervised learning (i.e., learning with given labels). 
\begin{thebibliography}{99}
 \bibitem{Broman}
 		Broman, Sen
 		\emph{A Guide to QTL Mapping with R/qtl} Springer
	
  \bibitem{Leisch}
	     Leisch
		\emph{FlexMix: A General Framework for Finite Mixture Models and Latent Class Regression in R}

\end{thebibliography}
\end{document}
