\documentclass[11pt]{article}
\usepackage[hmargin={0.8in, 0.8in}, vmargin={1in, 1in}]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{verbatim}
\usepackage{amsmath}                                                                                                                                                                                
\usepackage{amsthm}                                                                                                                                                                                 
\SweaveOpts{strip.white=all,keep.source=TRUE}
\SweaveOpts{include=TRUE,eps=FALSE,pdf=TRUE,width=8}
\begin{document}
\SweaveOpts{concordance=TRUE}

\noindent
\begin{flushleft}
\textbf{STAT 850 - Homework 3}\\
\textbf{Duy Nguyen}\\
\textbf{03-12-2014}
\end{flushleft}

\subsection*{Question 1: Complete 4f, 4g, 4h from Homework 2}

<<Question 1, echo=FALSE, results=hide>>=
Trough_00gm <- c(6.7, 7.8, 5.5, 8.4, 7.0, 7.8, 8.6, 7.4, 5.8, 7.0)
Trough_05gm <- c(9.9, 8.4, 10.4, 9.3, 10.7, 11.9, 7.1, 6.4, 8.6, 10.6)
Trough_10gm <- c(10.4, 8.1, 10.6, 8.7, 10.7, 9.1, 8.8, 8.1, 7.8, 8.0)
Trough_15gm <- c(9.3, 9.3, 7.2, 7.8, 9.3, 10.2, 8.7, 8.6, 9.3, 7.2)

Data <- stack(as.data.frame(cbind(Trough_00gm, Trough_05gm, Trough_10gm, Trough_15gm)))
names(Data) <- c('Hemoglobin', 'Trough')
Data <- within(data=Data,{
  Trough <- factor(Trough)
})
levels(Data$Trough) <- c('0 gm', '5 gm', '10 gm', '15 gm')
model1 <- aov(Hemoglobin ~ Trough, data=Data)
@
\subsubsection*{(f)}
<<echo=FALSE, results=verbatim>>=
TukeyHSD(model1, conf.level=.99)
@

As suspected, this reveals that  there are significant difference in means between treatments 1-2 and treatment 1-3.
\subsubsection*{(g)}
Compare the effect of no sulfamerazine on the hemoglobin content of trout blood with the average effect of the other three levels. The overall confidence level of all intervals should be at least 98\%.

If our goal is to control the CI to be above 98\%, we can use a 99\% CI to estimate the difference as constructed above. The reason is that the probability of one or more errors in the Tukey HSD is $0.01$ and the probability of error for this confidence interval is $0.01$, the probability of making an eror in either these twos is $1- P(no errors)= 1 - (1-.01)^2=0.0199.$ This shows that the overall confidence level of all intervals should be at least 98\%.

To construct this F-test, we use the contrast $c=(1,-1/3,-1/3,-1/3).$ This gives an F-value of $15.78 \sim F_{1,36}$ under $H_0:\mu_1 = \frac{\mu_1+\mu_2+\mu_3}{3}. $ The p-value is $0.0003.$ There is a significant evidence that the mean effect of the three non-zero treatments is not the same as the zero level treatment.
\subsubsection*{(h)}
The Tukey HSD margin of error is $Error = q(.95,4,4(r - 1)) * \frac{\hat{\sigma}}{\sqrt{r}},$ where $\hat{\sigma}^2=2.43$ can be taken as the upper bound on the 95\% CI. The length of CI is $2Error.$   
<<echo=TRUE>>=
r=31
2*qtukey(.95, 4, 4*(r-1))*sqrt(2.43)/sqrt(r)
r=32
2*qtukey(.95, 4, 4*(r-1))*sqrt(2.43)/sqrt(r)
r=33
2*qtukey(.95, 4, 4*(r-1))*sqrt(2.43)/sqrt(r)
r=34
2*qtukey(.95, 4, 4*(r-1))*sqrt(2.43)/sqrt(r)
@
Thus, to ensure that the width of CI is less then $2$, we should select a sample size of at least $33$ observations in each groups.

\subsection*{Question 2}
<<Q2, echo=FALSE, results=hide, fig=TRUE>>=
T300 <- c(5.1, 5.2, 5.8, 4.0, 5.5, 4.7, 5.5, 4.3)
T350 <- c(6.5, 8.2, 2.0, 6.1, 8.0, 4.3, 7.2, 10.6)
T400 <- c(5.9, 4.1, 6.4, 5.4, 3.7, 5.5, 7.6, 7.5)
T450 <- c(11.2, 9.8, 13.6, 12.7, 15.1, 8.8, 13.0, 12.7)
T500 <- c(13.8, 16.1, 18.9, 15.7, 17.0, 15.1, 17.3, 17.0)

Data <- stack(as.data.frame(cbind(T300, T350, T400, T450, T500)))
colnames(Data) <- c('Corrosion', 'Temp_Chr')
Data$Temp_Num <- as.numeric(gsub(pattern='T', replacement='', x=Data$Temp_Chr))
layout(matrix(c(1,2),nrow=1,ncol=2))
boxplot(Corrosion ~ Temp_Chr,data=Data, xlab="Temp.", ylab="Corrosion", main="Boxplot of 
        corrosion by Temperature")
boxplot(log(Corrosion) ~ Temp_Chr,data=Data, xlab="Temp.", ylab="Corrosion", main="Boxplot of 
        log(corrosion) by Temperature")
@

Based on the boxplot, we can see that equal variance assumption across 5 groups of temp. is violated. Log transform of corrosion looks a little better. Honestly, I cannot tell the difference. Thus, we need to look at the residual plots after our analysis.

<<Analysis, echo=FALSE, results=verbatim>>=
Model1 <- lm(Corrosion ~ factor(Temp_Chr, ordered=T), data=Data )
anova(Model1)
summary(Model1)
Model2 <- lm(log(Corrosion) ~ factor(Temp_Chr, ordered=T), data=Data )
anova(Model2)
summary(Model2)
@

Based on our results, we can see that the model without log transform fits better. The followings are residual plots

<<Residual, echo=FALSE, fig=TRUE>>=
layout(matrix(c(1,2),nrow=1,ncol=2))
plot(Model1$fitted, Model1$res, xlab="fitted values", ylab="residuals", main="Residual plots for model without log")  
plot(Model2$fitted, Model2$res, xlab="fitted values", ylab="residuals", main="Residual plots for model with log")  

@

The residual plot of our standard model (without transformation) looks a little better than the other one. For the log model, residuals have a tendency to get smaller. Therefore, I will use my standard model. As is evident from our R summary, all the polynomial coefficients are significant except for the cubic term.    

Based on the standardized residuals, observation $16$ seems to be an outlier (observation 3 in the group with temperature $350$ degrees).
<<outlier, echo=TRUE, results=verbatim>>=
Data2=Data[-16,]
Model1.rm <- lm(Corrosion~ factor(Temp_Chr, ordered=T), data=Data2)
@

\subsection*{Question 3}
\subsubsection*{(a)}

<<input_data, echo=FALSE, fig=TRUE>>=
library(ggplot2)
Data <- data.frame(N_Level=factor(rep(c(1,1,2,3,4,4),c(4,4,4,4,4,4))),
                   Irrigation=factor(rep(c("N","Y","N","N","N","Y"),c(4,4,4,4,4,4))),
                   BigBluestem_Frac=c(97,96,92,95,
                                      83,87,78,81,
                                      85,84,78,79,
                                      64,72,63,74,
                                      52,56,44,50,
                                      48,58,49,53)
                   )
Data$BigBluestem_Frac <- Data$BigBluestem_Frac/100
layout(matrix(c(1,2),nrow=1,ncol=2))
boxplot(BigBluestem_Frac ~ N_Level*Irrigation, ylab="Fraction of Big Bluestem",
        main="Response",data=Data)
boxplot(BigBluestem_Frac^2 ~ N_Level*Irrigation, ylab="Fraction of Big Bluestem",
        main="Response^2",data=Data)
#ggplot(data = Data, aes(x = N_Level, y = BigBluestem_Frac)) +
#  geom_boxplot(aes(fill = Irrigation), width = 0.8) + theme_bw()
@

Based on the boxplot (response), the variance of the data is somewhat different for different treatment groups. It gets smaller as the response increases. Since the response is proportion, we should apply logit(). However, this does not fit the problem. I tried to square the response. The boxplot shows that this transformation seems to fix the problem. The Barlett test for 

<<Barlett, echo=TRUE,results=verbatim>>=
bartlett.test(BigBluestem_Frac^2 ~ factor(paste(N_Level, Irrigation)), data=Data)
@

Again, with so few replications per group, it is not clear whether there is a violation of homogeneity or not.
\subsubsection*{(b)}
First, let consider our usual 2-way ANOVA model.

<<Models, echo=TRUE,results=verbatim>>=
Model.Anova <- lm(BigBluestem_Frac^2 ~ N_Level * Irrigation ,data=Data)
anova(Model.Anova)
@

Based on the anova table, we can see that the interaction $N.Level:Irrigation$ is very significant. Also, we cannot consider this as a 1-way ANOVA model with $6$ treatmens. Therefore, all the treatments are not equivalent. I also tried the data without transformation. It also yiels the same result.

Second, let consider N.Level as ordered factor and we want to test linear, quadratic, and cubic effects

<<Models_order, echo=FALSE, results=verbatim>>=
Model.order <- lm(BigBluestem_Frac^2 ~ factor(N_Level, ordered=T) * Irrigation, data=Data )
anova(Model.order)
@

As before, all effects including interactions are very significant. Thus, it is a little evidence that all the treatments are the same. I also tried the data without transformation. It also yiels the same result.
\subsubsection*{(c)}
<<Quad, echo=FALSE,results=verbatim>>=
Model3.T <- lm(BigBluestem_Frac^2 ~ factor(N_Level, ordered=T), data=subset(Data, Irritation='N') )
summary(Model3.T)
@

Based on this result, we do not see any significant quadratic effect of nitrogen level. 
\subsubsection*{(d)}
Based on the results in (b), we conclude that irrigation does have a significant effect on the response.

\subsubsection*{(e)}
<<Additive_model, echo=FALSE,results=hide>>=
Data$NewFactor <- factor(paste(Data$N_Level, Data$Irrigation))
str(Data)
Anova2.T <- aov(BigBluestem_Frac ~ NewFactor, data=Data )
Means <- aggregate(x=Data$BigBluestem_Frac, by=list(Data$N_Level, Data$Irrigation), FUN=mean)
#fit.contrast(model=Anova2.T, coeff=c(5, -1, -1, -1, -1, -1), varname='NewFactor')

@
By just looking at the mean in each columns of our data, we can see that column 1 (N.Level at level 1 and Irrigation at level N) has the highest mean response $95\%.$ In order to test significant, an F-test is conducted using linear contrasts.

$H_0 : \mu_1 = \frac{\mu2+\mu3+\mu4+\mu5}{5}$

$F_C\sim F_{1,18}$ with $c=(5,-1,-1,-1,-1,-1).$ This gives p-value less than $\alpha=0.05.$ Thus, there is a strong evidence that treatment $1 N$ has highest mean response. 

However, before conducting this F-test and having this conclusion, we need some conditions. To conduct a test for contrast between 6 groups $1N, 1Y, 2N, 3N, 4N, 4Y$, we need to assume that there are not interactions between N.Level and irrigation. Thus, we have 1-way model. 

Let's consider the model in part (b)
<<echo=FALSE,results=verbatim>>=
summary(Model.Anova)
@

The only significant interaction is N.Level4:IrrigationY. If we are willing to make an assumption that this interaction is not significant, we can obtain the above conclusion. 

\subsection*{Question 4}
Let $F_c$ denote the critical value under $H_0.$ Thus, power $=P_{H_a} (F\geq F_c),$ which is under a non-central F. The same result also holds for t-test. Clearly, if the critical values from two tests $F_c^1 < F_c^2,$ then test 1 is more powerful the test 2. 

We have $t=5$ groups and $df_{err}=25$ with $n=30.$ Thus, $df_{tr}=4.$ For Bonferroni,there are $m=10$ possible contrasts. For each contrast, under $H_0$, the critical value $t_c = t_{\alpha/m,n-t}.$ For Scheffe's method, under $H_0, F_c = F_{\alpha,t-1,n-t}(0).$ Comparing the powers $P_{H_a} (T > t_c)$ and $P_{H_a} (F > F_c),$ we conclude that upto $18$ contrasts.

<<output, echo=FALSE,results=verbatim>>=
Output <- NULL
M <- 2
for(M in 15:25){
  Crit_BF <- qt(p=0.05/(2*M), df=25, lower.tail=FALSE)
  
  #Crit_BF.F <- qf(p=0.05/(2*M), df1=1, df2=25, lower.tail=FALSE)
  
  Crit_Scheffe <- sqrt(4*qf(p=0.05, df1=4, df2=25, lower.tail=FALSE))
  Output <- rbind(Output, c(M, Crit_BF, Crit_Scheffe))
}
colnames(Output) <- c('Group Size', 'Bonferroni T-crit', 'Scheffe F-crit')
Output
@

\subsection*{Question 5}
\begin{figure}[!ht]
\centering
\includegraphics[height=9cm,width=9cm]{5-1.png}
\caption{}\label{fig:}
\end{figure}
\begin{figure}[!ht]
\centering
\includegraphics[height=9cm,width=9cm]{5-2.png}
\caption{}\label{fig:}
\end{figure}

As can be seen in the histogram above, the response variable has a lot of zeroes. From the boxplots, it seems that the variances between treatments are unequal. Thus, homogeneity of variance assumption is violated.  Below is a boxplot of the responses, after square-root transformation. It looks a bit better in terms of equal variance.
However, treatments 0 and 8 seem to exhibit higher variances.
\begin{figure}[!ht]
\centering
\includegraphics[height=9cm,width=9cm]{5-3.png}
\caption{}\label{fig:}
\end{figure}



\begin{figure}[!ht]
\centering
\includegraphics[height=9cm,width=9cm]{5-4.png}
\caption{Boxplot after removing zeroes}\label{fig:}
\end{figure}

Below are the means and standard deviations:
\\
\begin{tabular}{c c c }               % centered columns (17 columns) 
\hline\hline                            %inserts double horizontal lines 
%Case & Method\#1 & Method\#2 & Method\#3 \\ [0.5ex] % inserts table 
%heading 
Treatment & Mean & Std. Dev \\
0 & 1.0000 & NA \\
1 & 3.0673 & 0.9001 \\
2 & 1.4500 & 0.5262 \\
3 & 2.0893 & 1.5145 \\
4 & 1.6980 & 0.6993 \\
5 & 2.0206 & 0.8504 \\
6 & 2.0200 & 0.3870 \\
7 & 1.2866 & 0.3554 \\
8 & 3.5079 & 1.1225 \\
9 & 2.0987 & 0.6272 \\
10 & 2.4004 & 0.9041 \\
20 & 1.5486 & 0.9502 \\[1ex]             % [1ex] adds vertical space 
\hline                                  %inserts single line 
\end{tabular} \\

Below is the resulf of the overall F-Test, testing the difference between treatments:
\begin{verbatim}
            Df Sum Sq Mean Sq F value Pr(>F)  
Treatment   11  21.91  1.9917   2.627 0.0138 *
Residuals   37  28.05  0.7581                 
---
\end{verbatim}

An excess of zeroes will tend to produce biased estimates. Discarding the zeroes is not 
the best approach. More information should be acquired regarding those data points. A decision should be made regarding their methods of data collection.

\begin{itemize}
\item[(a)]
\end{itemize}





\subsection*{Question 6}

<<echo=FALSE, results=hide>>=
Data <- data.frame(GelStrength = c(62.9, 110.3, 60.1, 147.6, 
                                    44, 115.6, 57.9, 180.7, 
                                    43.8, 123.4, 58.2, 183.8, 
                                    34.4, 53.6, 63, 92), 
                   StarchSource = factor(rep(c("Bean", "Corn", "Wheat", "Potato"), 
                                              c(4, 4, 4, 4))), 
                   StarchConc = factor(rep(c('5 percent', '7 percent', '5 percent', '7 percent'), 4)),
                   Temp = factor(rep(c(22, 22, 4, 4), 4))
)
@

\begin{figure}[!ht]
\begin{minipage}[b]{0.4\textwidth}
\centering
\includegraphics[height=7cm,width=7cm]{6-1.png}
\caption{Original Data}\label{fig:*1}
\end{minipage}
\hfill
\begin{minipage}[b]{0.4\textwidth}
\centering
\includegraphics[height=7cm,width=7cm]{6-2.png}
\caption{Log Transform}\label{fig:*2}
\end{minipage}
\end{figure}

The plots show that we need to transform our data to obtain homogeneity in variance. I used log(GelStrength). The resulting plot looks better.

The first attempt is to fit the model with main effects and all interactions

lm(log(GelStrength) ~ StarchSource * StarchConc * Temp, data=Data)

This results to a perfect fit model with no degree of free for error. Thus, we cannot perform any inference. By Hierachical principal, I assume three-interactions are not significant. 
I perform the following model:

<<Models, echo=TRUE, results=hide>>=
Model2.log <- lm(log(GelStrength) ~ StarchSource + StarchConc + Temp + 
                   StarchSource:StarchConc + StarchConc:Temp + StarchSource:Temp, data=Data)
anova(Model2.log)
@

From the anova table, interactions of Temp with other factors are not significant. I performed an F-test to confirm this.

<<echo=TRUE,results=hide>>=
red = lm(log(GelStrength) ~ StarchSource + StarchConc + Temp + 
                   StarchSource:StarchConc , data=Data)
anova(red,Model2.log)
@

p-value=0.1268. Thus, we can drop interactions of Temp with other factors. I arrive at final model

<<echo=FALSE,results=verbatim>>=
anova(red)
@

\includegraphics[height=8cm,width=12cm]{6-3.png}

Residual plot seems fine except a slight decrease in variance as fitted values increase. Again, with few observations, anything can happen. The R-squared is 0.966.

As seen from R output, all three factors and interaction StarchSource:StarchConc are significant. Since there still exists an interaction in our model, it is not conclusive to say that how response changes if we change a level of one factor keeping other factors fixed. 


\includegraphics[height=20cm,width=15cm]{6-4.png}


\end{document}





